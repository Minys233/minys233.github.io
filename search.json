[{"categories":["随便写写"],"content":"Merge Two Dicts with overwriting Problem:\nI have 2 Python dictionaries, x = {...} and y = {...}, that may or may not have same keys. I want to merge them, and if there are identical keys, use values from y to overwrite ones in x.\nSolution:\nIn Python 3.9.0a4 or greater, according to PEP-584.\n1  z = x | y   In Python 3.5 or greater, this can be done by simply:\n1  z = {**x, **y}   In Python 2\n1 2 3 4  def merge_dicts(x, y): z = x.copy() z.update(y) return z   ","description":"","tags":["Python"],"title":"Coding Tricks - Python","uri":"/posts/python-tricks/"},{"categories":["学习笔记"],"content":"写在前面 为什么要学习JS呢? 对我来说, 日常科研工作中有很多场景都需要和互联网, 尤其是前端打交道. 比如:文献制图, 数据可视化, 训练监控, 维护博客等等. 在这些过程中有很多端到端的工具可以使用, 但也仅仅是停留在\"使用\". 有时出现错误或新增内容时, 总要写点新东西或者整合一些JS库. 每到这时, 总是会触及到我的知识盲区, 即\"明明知道用JS可以轻松解决但就是不会\". 列举几个让我十分沮丧的时刻:\n 本科时每学期评教, 要选中页面中所有特定的单选框 咪咕音乐PC端仅提供网页应用, 或许可以使用Electron打包成APP Clash科学上网, 可以用用HTML+JS可以包装官方简陋的RESTful API Gnome更新后, 某些插件不能用了但无法debug  因此, 我决定在闲暇之时, 抽出时间系统地学一下JS. 但是学归学, 为什么要开这个系列坑呢? 因为之前我已经学过好几次JS了! 每一次要么是疏于练习前学后忘, 要么是没有紧迫需求无法坚持. 我希望这个博问可以督促我每天学一点, 坚持下去.\n这个系列突出实用, 即默认读者应当已经掌握了一定的编程/互联网基础, 太过基础的和概念并不会占用太多篇幅. 本教程参考廖雪峰的教程编写, 并尽可能短小精悍.\nJavascript历史简述 微软等几个大公司在1997年联合制定了ECMAScript标准, Javascript便不断跟进ES标准开发并实现. 如今现代浏览器所支持地最新Javascript标准是于2015年6月发布的ES6版本. 这个ES也正是前述ECMAScript的缩写. 其他的历史不了解也罢, 但是ES标准最好还是了解一下, 因为各种各样的浏览器在执行Javascript时所遵循的标准并不相同, 例如远古的IE6和最新版的Chrome便不同, 前者并不支持ES6标准的Javascript代码, 而且, 即使时最新版的不同浏览器, 如Safari与Firefox, 也会在执行Javascript脚本时有不一样的行为.\n基本语法和基本类型 语句和注释 一个编程语言开始上手首先就要关注它的语法, 基本类型, 这里也不例外. Javascript的语法类似C++或Java, 但却是一个动态类型语言. 其基本的运行单元是语句. 一个语句要以 ; 结尾, 一个语句块要处在 {...} 中. 但由于某些原因, Javascript并不强制加 ; , 浏览器中JS引擎会自动给没有分号的语句加上分号. 但是有时却会导致预期之外的效果. 因此, 每个语句结尾手动添加分号是良好的代码习惯.\n语句样例:\n1 2 3  var a = 1; var $b = 2; 'Hello Javascript!';   Javascript中的注释沿用了C风格注释, 即 // 表示单行注释, \\* ... *\\可注释多行.\n基本类型 数字 (Number) Javascript不区分数字类型, 整数, 浮点统一表示. 数字类型中有两个特殊值NaN和Infinity. 其意义见如下注释:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  123 // 整数 1.234 // 浮点数 1.234e5 // 123400的科学计数法 0xffff00 // 十六进制数, 同对应十进制数值 NaN /* Not a Number, 计算失败时出现NaN. 如Math.sqrt(-1), parseInt(\"f*ck\"), 'a' + 1, 0/0. 详细来说, 无穷大除以无穷大, 给任意负数做开方运算, 或者算数运算符与不是数字或无法转换为数字的操作数一起使用时都将返回NaN. */ Infinity /* 无穷, 分为正无穷 (Infinity或+Infinity) 和负无穷 (-Infinity). 它们都比任何数大/小. 对有限数除以0 (区分0/0) 或者超出Number.MAX_VALUE可以 得到Infinity, 另外, Infinity参与的四则运算都会得到±Infinity. 但是, 注意: parseInt(\"Infinity\", 10) =\u003e NaN, Math库中某些函数 也会返回Infinity, 如: const empty=[]; Math.max(...empty); ==\u003e Infinity Math.min(...empty); ==\u003e -Infinity */   字符串 (String) 字符串使用 '' 或 \"\" 包围的任意文本, 如 abc , \"xyz\" . 字符转义与其他语言完全一致, 如 'I\\'m Yaosen!\\n'. 另外,也可以使用十六进制数 '\\x##' 表示字符串中的字符, 如 '\\x41bc' === 'Abc'. 也可以用 '\\u####' 表示字符串中的一个Unicode字符, 如 '\\u4e2d\\u6587' === '中文'.\n如果字符串很长, 写换行符会很烦, 可以使用ES6新标准中的多行字符串表示方法, 用`...` 括住即可. 同时, 反引号字符串也可以当作模板字符串将变量嵌入至字符串中, 而不用写很多+号.\n1 2 3 4 5 6 7 8 9  var name = 'Yaosen'; var age = 24; var message1 = 'Hello, I\\'m ' + name + \", I'm \" + age + \" years old.\" // messagge1 === \"Hello, I'm Yaosen, I'm 24 years old.\" var message2 = `Hello, I'm ${name}, I'm ${age}years old.` // messagge2 === \"Hello, I'm Yaosen, I'm 24 years old.\" var message3 = `Hello, I'm ${name}, I'm ${age}years old.` // message3 === \"Hello, I'm Yaosen,\\nI'm 24 years old.\"   字符串对象的操作类似列表. 通过 .length 获得长度, 通过方括号索引获取对应字符. 但是修改 .length 虽不会报错, 但是并不能更改字符串长度, 属性值也不会改变. 字符串是不可变的, 对索引赋值不会有任何效果.\n1 2  var s = \"Hellw!\"; // mis-spelled s[4] = 'o'; // no effect   个人认为, 字符串操作的丰富与否直接决定了一个语言是否对新手友好, 因为如果打log都很不是很容易, 那么学习曲线就会很陡峭. 因此, 易于学习的Javascript字符串对象有不少成员函数来操作字符串:\n s.toUpperCase() : 全部大写, 仅对有大小写关系的字符有效. s.toLowerCase() : 全部小写, 仅对有大小写关系的字符有效. s.indexOf(str) : 返回指定 str 字符串出现的位置, 没找到返回-1. s.substring(start, end) : 返回 [start, end) 区间的子字符串, 如 end 未指定, 默认到字符串结尾.  布尔值 同C/C++, 有 true , false 两种值. 对应的布尔运算如下:\n1 2 3  true \u0026\u0026 true // 逻辑与 true || false // 逻辑或 !false // 逻辑非   比较运算符 包含 \u003e, \u003c, \u003e=, \u003c=, ==, ===. 前面四种略去不说, 字如其意. 最后两种比较相等的运算符具有不同的行为, 要特别注意.\n == : 比较时自动转换类型, 可得到 false == 0 为 true 这样的神奇情况 === : 比较时不会转换类型,如果数据类型不一致直接 false, 类型相同再比较  这是Javascript的一个设计缺陷, 尽量避免使用 ==. 另外, NaN 和任何数包括他自己都不想等, 即 NaN === NaN 将得到 false, 符合数值运算时约定俗称的规矩. 实际上, === 当且仅当两边的对象指向相同的数据, 如 new Number(0) === new Number(0); 为 false.\nnull 与 undefined null 表示空 (void) 之意, 类似C中的 NULL, Python中的 None. undefined 表示未定义. 大部分情况下,都应该用 null, 仅在判断函数传参的时候使用 undefined.\n数组 (Array) 数组是一个有序的元素序列, 可以包含任意类型元素, 一般使用以下两种方式定义. 使用整数索引可以访问元素的\"引用\". 不同于Python, Javascript并不支持能通过方括号索引进行负整数索引与数组切片操作.\n1 2 3 4 5 6 7  var arr = [1, 2, 3.14, 'hello', null, true]; var arr1 = new Array(1, 2, 3.14, 'hello', null, true); arr[0]; // 1 arr[5]; // true arr[6]; // undefined arr[-1]; // undefined arr[0] = 2 // change to [2, 2, 3.14, ...]   访问数组的 .length 属性可以得到列表长度. 注意: 更改 .length 属性会使得数组丢失元素或增加 undefined 元素. 如果你在浏览器中试过, 你会发现似乎显示的是 empty. 但其实这是undefined. 另外, 除了上面一点外, 不同于字符串的还有, 当索引值越界后, 数组会自动变长,并在中间添加 undefined.\n和字符串类似, 字符串也支持一系列成员函数如下所示.\n .indexOf(Object) : 搜索一个指定元素 Object 的下标, 没有找到时返回-1. Array.slice(start, end) : 子数组操作. 对应的是字符串的 substring 函数, 截取数组的一段元素. 如果不传递参数, 则会截取所有元素, 通常用于复制一个数组. 注意, 这里的复制规则比较复杂, 可以先这么记忆, 但实际上只会真正复制字符串和数字和布尔值, 详见下图, deep表示复制内容, shallow表示只复制一个引用. push(*arg) : 可传递任意多参数, 所传递的参数将按照顺序向末尾添加新元素. 该函数返回目标数组的新长度. pop() : 把最后一个元素删掉, 返回删除的元素. unshift(*arg) : 类似 push, 向头部添加元素. shift() : 类似 pop, 删掉第一个元素, 返回删除的元素. sort() : 按照递增字典序排序, 也可以传递一个比较函数, 暂且不提. reverse() : 翻转数组顺序. splice(start, num, *arg) : 删除+插入. 从 start 位置自己开始, 删除 num 个元素, 并在这里插入 arg. 该函数返回删除的元素. concat(*arg) : 当前的 Array 和另一个 Array 连接起来, 并返回一个新的 Array. 注意, 该函数并没有修改当前 Array, 而是返回了一个新的 Array. 该函数可以接受任意多参数, 如果参数包含数组, 也会将数组拆开加入. join(str) : 将每个元素间用 str 连接起来, 然后返回连接后的字符串.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  var arr = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'G']; arr.indexOf('G'); // 6, the first appearance arr.slice(1,3); // [\"B\", \"C\"] arr.slice(3); // from arr[3] to end, [\"D\", \"E\", \"F\", \"G\", \"G\"] arr.slice(); // make a copy, [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"G\"] arr.slice() === arr; // false, not the same object  var arr = ['Microsoft', 'Apple', 'Yahoo', 'AOL', 'Excite', 'Oracle']; // 从索引2开始删除3个元素,然后再添加两个元素: arr.splice(2, 3, 'Google', 'Facebook'); // 返回删除的元素 ['Yahoo', 'AOL', 'Excite'] arr; // ['Microsoft', 'Apple', 'Google', 'Facebook', 'Oracle'] // 只删除,不添加: arr.splice(2, 2); // ['Google', 'Facebook'] arr; // ['Microsoft', 'Apple', 'Oracle'] // 只添加,不删除: arr.splice(2, 0, 'Google', 'Facebook'); // 返回[],因为没有删除任何元素 arr; // ['Microsoft', 'Apple', 'Google', 'Facebook', 'Oracle']  var arr = ['A', 'B', 'C']; var added = arr.concat([1, 2, 3]); added; // ['A', 'B', 'C', 1, 2, 3] arr; // ['A', 'B', 'C'] var arr = ['A', 'B', 'C']; arr.concat(1, 2, [3, 4]); // ['A', 'B', 'C', 1, 2, 3, 4]  var arr = ['A', 'B', 'C', 1, 2, 3]; arr.join('-'); // 'A-B-C-1-2-3'   对象 Javascript的对象像C++ STL中的 unordered_map 或 Python中的 dict, 是由一组key-value对组成的无序集合,通过索引键来访问值. 键值对末尾除了最后一个外必须加逗号, 如果最后一个也加了逗号, 低版本浏览器有可能会报错.\n1 2 3 4 5 6 7 8 9  var person = { name: 'Bob', age: 20, \"school-name\": 'Tsinghua', tags: ['js', 'web', 'mobile'], city: 'Beijing', hasCar: true, zipcode: null };   但不同的是, javascript中对象的键都是字符串类型, 值 (属性) 可以是任意类型. 如果属性名包含特殊字符, 则属性名要用单引号或双引号括起来. 要访问某个属性的值, 需通过 . 来完成, 如果属性名包括特殊字符则只能通过方括号来访问.\n1 2 3 4  person.name; // 'Bob' person.zipcode; // null person['name']; // 'Bob' person['school-name']; // 'Tsinghua'   因为Javascript是动态类型语言, 因此可以动态地添加或删除对象的属性. 但是注意, 如果访问到一个不存在的属性, 将会返回 undefined. 检测一个对象是否拥有某个属性可以使用 in 操作符, 但 in 操作符同样也会对继承而来的属性进行判断, 即有可能这个属性是继承而来的而非它自身的, 如要判断是否是其自身的属性, 需要用 hasOwnProperty().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  var xiaoming = { name: '小明' }; xiaoming.age; // undefined xiaoming.age = 18; // 新增一个age属性 xiaoming.age; // 18 delete xiaoming.age; // 删除age属性 xiaoming.age; // undefined delete xiaoming['name']; // 删除name属性 xiaoming.name; // undefined delete xiaoming.school; // 删除一个不存在的school属性也不会报错 'name' in xiaoming; // true 'school' in xiaoming; // false 'toString' in xiaoming; // true xiaoming.hasOwnProperty('toString'); // false   变量 其实以上的代码示例中已经用到了很多变量. Javascript中的变量名可以由字母, 数字, $, _组合而成, 但不能用数字开头. 当然, 变量名也不能是语言自身的关键字. 声明并定义一个变量的推荐方式为 var name = ...; 形式.\n其实, javascript并不要求变量使用 var 声明. 但是, 如果一个变量没有使用 var, 那么他将变成一个全局变量 (如 b ). 这样会使得同一个页面引入的不同JS文件中的变量相互冲突, 造成不可预知的后果. 因此, 牢记声明变量的时候应当使用 var.\n1 2 3  var a = 1.23; a = 'string'; b = 'I\\'m conflict with other bs'   这个问题实际上是javascript的一个设计缺陷, ECMA在后续规范中推出了strict模式, 在该模式下运行的代码强制通过 var 声明变量, 不加 var 的变量声明将直接抛出错误. 启用strict模式的方法是在脚本中首行写入:\n1  'use strict';   支持strict模式的浏览器将开启strict模式, 而不支持该模式的浏览器将会把这一行当作一个没有什么意义的普通语句来执行. 为了避免因为粗心大意在声明变量时忽略 var, 任何脚本都应该开启strict模式.\n阶段总结 这一块讲了基本的javascript的内置类型和基本的语法, 记忆的东西偏多, 基本没有需要重点理解的地方. 可以看到内容也不是很多, 因此说学习javascript学习曲线很平缓. 但是, 易学并不代表易写, 也不表示轻松就能写好, 还需要多多努力.\n","description":"","tags":["Javascript"],"title":"Javascript学习笔记 (1)","uri":"/posts/javascript%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"},{"categories":["学习笔记"],"content":"写在前面 通过上一节的学习, 已经基本了解了Javascript的基本类型, 程序的逻辑必然离不开条件判断和基本函数. 另外, 在基本类型之上的一些高级类型会极大地提高代码速度和效率. 因此这一节就主要讲条件判断, 循环和两个高级类型和它们的拓展内容.\n条件判断 Javascript的条件判断和C/C++中的一模一样, 如下面代码片段所示. 其中大括号可以略去, 但作用范围就会仅包含到下一个分号处. 同理, if() {} else if() {} else {} 类型的多条件判断也是可行的.\n1 2 3 4 5 6  var age = 20; if (age \u003e= 18) { alert(\"本站不允许未成年人访问!\"); } else { alert(\"本站建立于美国, 受美国法律保护, 服务在美华人.\"); }   需要注意的是, Javascript将 null, undefined, 0, NaN, 空字符串均视为 false, 其他值均视为 true.\n循环 基本循环也和C++一样, 分 for 循环和 while 循环, 语法格式也没有变化. 循环中使用 break 跳出, 使用 continue 进入下一轮.\n1 2 3 4 5 6  var arr = ['Apple', 'Google', 'Microsoft']; var i, x; for(i=0; i\u003earr.length; i++) { x = arr[i]; console.log(x); }   可以看到这样循环是有点不太优雅, 需要提前定义变量. 实际上可以在循环中定义变量. 另外, 循环常常用作循环一个对象的键值, 可以用类似Python的 for ... in 语法. 而数组也是一个对象, 它的元素的下标即被视为对象的属性, 也可以使用这种方式. 但是, 特别注意, for ... in 对数组使用时, 循环中得到的变量是字符串形式, 并非数字. 这也说明, 用数字字符串也可以作为下标访问数组元素.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  var arr = ['Apple', 'Google', 'Microsoft']; for(var i=0; i\u003earr.length; i++) { console.log(arr[i]); } for(var i in arr) { // NOTE: i here is '0' '1' '2', not Number  console.log(arr[i]); } var o = { name: 'Jack', age: 20, city: 'Beijing' }; for (var key in o) { if (o.hasOwnProperty(key)) { console.log(key); // 'name', 'age', 'city' 不加判断效果一样  } }   while 循环和C++中的基本也完全一样, 直接看代码即可. 类似地, 还有 do ... while 循环, 即先做 do 再判断, 后置条件判断.\n1 2 3 4 5 6 7 8 9 10 11  var x = 0; var n = 99; while(n \u003e 0) { x = x + n; n = n - 2; } var n = 0; do { n = n+1; } while (n \u003c 100)   特别注意! 在浏览器中练习循环的时候, 一定要多多注意边界条件, 不要写死循环. 不然会直接卡死浏览器只能强退再开. 不要问我怎么知道的.\nMap和Set ","description":"","tags":["Javascript"],"title":"Javascript学习笔记 (2)","uri":"/posts/javascript%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"},{"categories":["杂记"],"content":"The Gelato Bet 本节翻译自The Gelato Bet.\n打赌在过去的日子中是科学圈中的趣事. 在牛津剑桥 (oxbridge) 大学的高级公共休息室(SCR)中, 我们仍然可以找到旧赌本, 上面记录着牛津剑桥教员们打赌的历史, 读起来非常有趣. 在伯克利, 我们试着坚持这样的传统, 只是不是在烟雾缭绕的休息室, 而是在Nefli咖啡馆 (令人难过的是, 现已不在存在). 接下来打得这个赌发生在2014年9月23日, 三位公证人 (Kateria Fragkiadaki, Philipp Krähenbühl, and Georgia Gkioxari, 照片见原链接) 面对面手握手打的赌.\n“If, by the first day of autumn (Sept 23) of 2015, a method will exist that can match or beat the performance of R-CNN on Pascal VOC detection, without the use of any extra, human annotations (e.g. ImageNet) as pre-training, Mr. Malik promises to buy Mr. Efros one (1) gelato (2 scoops: one chocolate, one vanilla).\"\n“如果, 在2015年秋天的第一天前 (2015年9月23日), 在不适用任何额外的, 人工标注数据 (如 ImageNet) 的预训练下, 出现一种方法能够在Pascal VOC检测任务性能上追平或击败R-CNN的话, Malik先生允诺为Efros先生买一个 glato (一种意大利冰淇淋), 冰淇淋有两个球, 一个巧克力味, 一个香草味. “\n打这个赌的背景故事如下. R-CNN在CVPR 2014上被提出, 并在PASCAL VOC检测任务上拥有令人印象深刻的性能. 我认为这是计算机视觉社区中比较怀疑深度学习的成员 (包括我自己) 最终拥抱深度学习的关键时刻. 然而, 这其中还有一个难题: 据称, PASCAL VOC数据量太小, 无法从头训练出一个卷积网络, 因此, 网络不得不现在ImageNet上预训练, 然后在PASCAL上微调 (fine-tune). 这对我来说很是奇怪: PASCAL和ImageNet数据集是差异很大的数据集, 其中的标签集和重点完全不同… 为什么在已个数据集上训练会有利于另一个数据集上的性能呢? 那天下午在Nefli喝咖啡的时候, 我提出或许网络并不需要ImageNet的标签, 而仅仅需要图像用以预训练. 说穿了, 我想要回答的科学问题是: 学习好的表示需要语义监督吗? (does one need semantic supervision to learn a good representation?) 于是, Gelato Bet诞生了. 为了吸引其他的研究者参与这个赌局中来, 我承诺将分享我的glato冰淇淋给那些帮助我赢得这场赌局的研究团队.\n显而易见, 我输了, 即使是5年过去了, 我们依然没能够超越ImageNet预训练后的模型在PASCAL VOC任务上的性能 (虽然有几种方法距离超越仅一步之遥). 事实上, PASCAL任务需要预训练这个假定很可能一开始就是有问题的. 另一方面, 这个赌局对我们现在称之为的\"自监督学习\"在ICCV'15上的提出可能起了一定作用. 最后, 这件事给我上了宝贵的一课: 和自己导师打赌前一定要三思而后行!\nAlyosha Efros Berkeley, CA 2019年3月\n","description":"","tags":["Machine Learning"],"title":"机器学习轶事趣闻","uri":"/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%BD%B6%E4%BA%8B%E8%B6%A3%E9%97%BB/"},{"categories":["文献阅读"],"content":"论文简述 药物发现的目标是设计具有特定期望的化学性质的分子. 这个任务很具有挑战性, 因为化学空间是巨大的且难于探索. 一个解决该问题的流行的方法使匹配分子对分析 (MMPA). 通过学习\"分子释义\", 来提升化合物特定的性质. 这与机器翻译的策略很相似: MMPA输入一个分子对 $\\{(X, Y)\\}$, 其中 $Y$ 看做是 $X$ 的释义/含义, 并具有更好的化学性质. 然而现有的MMPA方法将分子对提炼为图转化 (graph transformation) 问题, 而非更普适的图之间的翻译问题.\n因此,本文提出将分子优化问题 (molecular optimization) 看做一个图对图的翻译 (graph-to-graph translation). 给定分子对的文集, 任务目标是学习如何将输入的分子图翻译为更好的图. 这个思路中涉及很多问题: 如何编码图, 如何生成图. 其中如何编码图已有若干工作, 但在不诉诸于专业领域知识的情况下生成图, 确是一个难题. 另外, 一个分子的\"释义\"可以是多个, 因为有不同的分子优化策略. 因此, 本文的问题最终转化为如何生成多模态的分子图.\n本文使用了junction tree encoder-decoder来在注意力机制下解码生成分子图. 为了 (a) 捕捉不同的输出, 本文在解码过程中引入了隐码 (latent code), 使其能捕捉有意义的分子变种; 为了 (b) 避免无效的翻译结果, 本文以入了对抗训练, 使用随机选择的隐码对齐模型生成的图的分布和以观察到的有效输出 (真值) 的分布.\n解决方案 Junction Tree Encoder-Decoder 本文的翻译模型拓展了junction tree variational autoencoder. 我们将每个分子看做从子图 (原子团/官能团) 构建而来, 基于一个合法的分子子结构库. Junction tree中的原子团代表着分子的骨架 (scaffold), 如下图所示. 分子解码的过程包括: 生成junction tree; 合并树中节点得到分子. 这样从粗到细的过程将允许我们很轻松的控制生成的图在化学上是正确的的, 并且能将分子在不同层级取得较为丰富的表示.\n模型的编码器包含: 一个图消息传递网络, 编码树和图至嵌入向量. 模型的解码器包含: (1) 树结构解码器, 用以预测junction tree的结构, (2) 一个图解码器, 用以将树扩充为分子图.\n树图二合一编码器 图定义为 $G=(\\mathcal{V}, \\mathcal{E})$. 图中节点 $v$ 具有特征 $\\boldsymbol f_v$ . 对原子来说, 其中包含了原子类型, 化合价等原子性质. 对junction tree中的节点 (文中称cluster), $\\boldsymbol f_v$ 是one-hot向量, 表示其类别. 类似地, 边 $(u,v)\\in\\mathcal{E}$ 也有对应的特征 $\\boldsymbol f_{uv}$. $N(v)$ 表示节点 $v$ 邻居构成的集合. 每条边 $(u,v)$ 有两个隐向量: $\\boldsymbol\\nu_{uv} , \\boldsymbol\\nu_{vu}$ , 分别表示两个方向传递的消息. 则消息传递网络通过神经网络 $g_1(\\cdot)$ 更新图中边上的消息:\n$$ \\boldsymbol\\nu_{uv}^{(t)}=g_1\\left(\\boldsymbol f_u, \\boldsymbol f_{uv}, \\sum_{w\\in N(u)\\backslash v} \\boldsymbol\\nu_{wu}^{(t-1)}\\right) $$\n其中 $\\boldsymbol \\nu_{uv}^{(t)}$ 表示第 $t$ 次迭代时边 $(u,v)$ 上的消息, 其初始化为零向量. 图上节点更新顺序是异步的, 即没有预定义的顺序. 在 $T$ 次迭代之后, 我们将图中消息通过神经网络 $g_2(\\cdot)$ 聚合, 得到每一个节点的嵌入向量, 其刻画了..图或树中的局部结构.. $$ \\boldsymbol x_u = g_2\\left( \\boldsymbol f_u, \\sum_{v\\in N(u)} \\boldsymbol \\nu_{vu}^{(T)} \\right) $$\n对junction tree $\\mathcal{T}$ 和分子图 $G$ 都使用消息传递网络编码,得到了 $\\{\\boldsymbol x_1^{\\mathcal{T}},\\cdots, \\boldsymbol x_n^{\\mathcal{T}}\\}$ 与 $\\{\\boldsymbol x_1^{\\mathcal{G}},\\cdots, \\boldsymbol x_n^{\\mathcal{G}}\\}$ .1\nJunction Tree解码器 这一步的目标是根据编码器输出的树表示和图表示重建junction tree. 这里使用了树循环神经(tree RNN)网络+注意力机制. 构建树的过程是自顶向下的, 每次拓展树的一个节点. 形式化地说, 令 $\\tilde{\\mathcal E}={(i_1,j_1),\\cdots,(i_m,j_m)}$ 为树 $\\mathcal{T}$ 的深度优先遍历, 其中 $m=2|\\mathcal E|$ 因为每个边从两个方向看要算两次. 令 $\\mathcal{\\tilde E_t}$ 为 $\\mathcal{\\tilde E}$ 中前 $t$ 个边. 在第 $t$ 步的解码中, 模型访问节点 $i_t$ 并接受其邻居的消息 $\\boldsymbol h_{ij}$ . 消息向量 $\\boldsymbol h_{i_t,j_t}$ 通过 树GRU更新: $$ \\boldsymbol h_{i_t,j_t} = \\text{GRU}(\\boldsymbol f_{i_t}, \\{\\boldsymbol h_{k, i_t}\\}_{(k, i_t)\\in \\tilde{\\mathcal{E}}, k\\ne j_t}) $$\n..拓扑结构预测... 当模型访问节点 $i_t$ 时, 首先通过一层神经网络编码节点特征 $\\boldsymbol f_{i_t}$ 和输入消息 $\\{\\boldsymbol h_{k, i_t}\\}$ 来计算计算隐状态 $\\boldsymbol h_t$. 模型随后进行二分类, 预测是否拓展这个新节点, 或是回溯回 $i_t$ 的父亲节点. 概率是通过聚合编码器所编码的两组嵌入向量 $\\{\\boldsymbol x_*^{\\mathcal{T}}\\}, \\{\\boldsymbol x_*^{\\mathcal{G}}\\}$ 而来的. $$ \\begin{aligned} \\boldsymbol h_t \u0026= \\tau(\\boldsymbol W_1^d\\boldsymbol f_{i_t}+\\boldsymbol W_2^d\\sum_{(k,i_t)\\in \\tilde{\\mathcal{E}}_t}\\boldsymbol h_{k,i_t}) \\\\\n\\boldsymbol c_t^d \u0026= \\text{attention}(\\boldsymbol h_t, {\\boldsymbol x_*^{\\mathcal{T}}}, {\\boldsymbol x_*^{\\mathcal{G}}}; \\boldsymbol U_{att}^d) \\\\\n\\boldsymbol p_t \u0026= \\sigma(\\boldsymbol u^d\\cdot\\tau(\\boldsymbol W_3^d\\boldsymbol h_t+\\boldsymbol W_4^d\\boldsymbol c_t^d)) \\end{aligned} $$ 其中 $(\\cdot;\\boldsymbol U_{att}^d)$ 表示参数为 $\\boldsymbol U_{att}^d$ 的注意力机制, 其在树和图上分别计算得到两组注意力分数 ${\\boldsymbol \\alpha_*^{\\mathcal{T}}}, {\\boldsymbol \\alpha_*^{\\mathcal{G}}}$ . 输出的 $\\boldsymbol c_t^d$ 是树和图的注意力加权向量的级联. $$ \\boldsymbol c_t^d = \\left[\\sum_i \\boldsymbol \\alpha_{i,t}^{\\mathcal{T}}\\boldsymbol x_{i}^{\\mathcal{T}}, \\sum_i \\boldsymbol \\alpha_{i,t}^{\\mathcal{G}}\\boldsymbol x_{i}^{\\mathcal{G}}\\right] $$\n..标签预测... 如果节点 $j_t$ 是 $i_t$ 生成的新节点, 其标签(表明它是何种分子骨架的标签)可以通过下式预测. $$ \\begin{aligned} \\boldsymbol c_t^l \u0026=\\text{attention}(\\boldsymbol h_{i_t,j_t}, {\\boldsymbol x_*^{\\mathcal{T}}}, {\\boldsymbol x_*^{\\mathcal{G}}}; \\boldsymbol U_{att}^l) \\\\\n\\boldsymbol q_t \u0026= \\text{softmax}(\\boldsymbol U^l\\cdot\\tau(\\boldsymbol W_1^l\\boldsymbol h_{i_t,j_t}+\\boldsymbol W_2^l\\boldsymbol c_t^l)) \\end{aligned} $$\n其中 $\\boldsymbol q_t$ 是在标签集上的概率分布, $\\boldsymbol U_{att}^l$ 是另一组计算注意力时的参数.\n图解码器 解码的第二步是从上一步预测的junction tree $\\mathcal{\\hat{T}}$ 出发, 构建分子图 $G$. 这个过程是非确定性的, 因为如下图所示, 同样的junction tree可以组装成不同的分子. 这一过程的自由度取决于原子团(树的节点)之间是如何连接的. 令 $\\mathcal{G}_i$ 为一个分子图集合, 表示节点 $i$ 能发生的可能的连接方式对应的分子.\n每一个分子图 $G_i \\in \\mathcal G_i$ 都表示原子团 $C_i$ 和其邻居原子团 $\\{C_j, j\\in N_{\\mathcal{\\hat{T}}}(i)\\}$ 的某种特定的连接方式. 这个图解码器的目标就是正确预测树中原子团的连接方式.\n为此, 本文作者设计了一个打分函数 $f(\\cdot)$ 来对每一个 $\\mathcal G_i$ 中的候选分子图 (对应连接方式) 进行排序. 首先, 用图消息传递网络对图 $G_i$ 计算原子表示 $\\{\\boldsymbol \\mu_v^{G_i}\\}$ . 然后使用sum-pooling求得图的表示 $\\boldsymbol m_{G_i}=\\sum_v \\boldsymbol \\mu_v^{G_i}$ . 最后, 通过点积函数为这个图打分 $f(G_i) = \\sum_{u\\in G}\\boldsymbol m_{G_i}\\cdot \\boldsymbol x_u^{\\mathcal{G}}$.2解码器训练过程的损失函数是真值的子图和树中节点(原子团)的对数似然函数: $$ \\mathcal L_g(G) = \\sum_i\\left[ f(G_i)-\\log\\sum_{G_i'\\in\\mathcal G_i}exp(f(G_i')) \\right] $$\n多模态图-图翻译 变分Junction tree编码-解码器(VJTNN) 将上述模块组织起来形成整个模型\n分子骨架的对抗正则化 这部分旨在让模型输出正确的分子结构, 通过一个GAN来规范化模型,使得其输出正确的分子结构.\n(未完待续)\n个人感受 这篇文章的模型实在是太过复杂, 已经读了两三天了, 但是依然感觉有一些雾里看花. 一个很模糊的点是模型提出的动机, 例如: 如何解决文章开头提到的多模态图生成问题. 为什么要用编码器解码器这样的结构? 为什么要使用junction tree这样的有序树结构来表示无序的图? 为什么要联合树和图一起编码?\n个人认为其中缺乏很多解释, 更多的是搭建一个如何能完成这个任务的模型, 试想我们把化学分子换成蛋白质结构的空间距离图, 是不是也能用? 如果这样的话那究竟它编码的过程都\"学\"到了什么呢? 我认为作者应该更多解释一下如何从数据上刻画分子表示, 根据什么想法设计模型, 而非大量篇幅描述复杂的模型, 但模型某模块为什么加入. 这样我认为即使有好的结果也很难去解释为什么好. 虽然我看在Table2里,本文提出的结果似乎并没有比前人工作好多少, 甚至似乎都没有超出随机波动 (QED success提升2.5%, diversity提高0.045, novelty并没有提高. 第二个任务每一项指标都提高小于1个百分点). 另, 相比于VSeq2Seq模型直接翻译分子间的SMILES, 复杂性提升很多的模型却没有带来显著的性能提升, 是否应该有一些思考.\n另外, 我的前导师崔老师教导我, 写公式的时候不到万不得已千万不要用连续下表, 比如 $i_{j_k}$, 很容易让读者迷惑. 我真真切切感受到了. 这些复杂的表示其实换一种想法完全可以省略, 比如 $\\boldsymbol \\nu_{uv}^{(t)}$, 完全可以并入公式 (2) 中. 感觉文章写完, 希腊字母似乎都被用光了. 这也可能是化生学科和计算机学科之间在认知上的鸿沟吧!\n综上, 这篇文章咬牙看完, 看到最后结果汇总并和基线模型差距不大甚至无法超越的时候, 突然就丧失了一半继续写下去的动力, 然后转身去看代码, 发现代码是Python 2.7 + pytorch 0.4 + rdkit 2017.09! 不说别的, 这个环境配起来确实不是很方便, 至此, 丧失了另一半写下去的动力.\n如果以后有机会/需求的时候, 再接着这里写完吧!\n  这里有问题. junction tree中的节点数目按理来说应该小于分子图, 因为树中一个节点代表一个子结构. 但这里公式却写着两个嵌入向量集合具有相同的向量个数, 可能有误! ↩︎\n 这个打分函数我强烈怀疑有问题. 竟然解码的结果会和编码的结果比较, 这不就相当于解码器模型的输出好坏取决于输入吗? 个人感觉这里有问题. ↩︎\n   ","description":"","tags":["Graph Auto Encoder (GAE)","Machine Learning","未完成"],"title":"Learning Multimodal Graph to Graph Translation for Molecular Optimization","uri":"/posts/learning-multimodal-graph-to-graph-translation-for-molecular-optimization/"},{"categories":["文献阅读"],"content":"Preliminary The Weisfeiler-Lehman Isomorphism Test For a easy-to understand explanation, please refer to this link. Here, I simply summarize it briefly.\n In general, determining whether two graphs are isomorphic when the correspondance is not provided is a challenging problem; precisely how hard this problem is remains an open question in computer science. It isn’t known whether there is a polynomial time algorithm for determining whether graphs are isomorphic, and it also isn’t known whether the problem is NP-complete. The graph isomorphism problem may even be an example of an NP-intermediate problem, but this would only be possible if $P\\ne NP$.\nFrom link above\n The Weisfeiler-Lehman Isomorphism Test produces canonical forms of graphs. If the canonical forms of two graphs are not same, then two graphs are not isomorphic. It is possible for two graphs to have same canonical forms but are not isomorphic. The algorithm iterative encode nodes (usually by a hash function) in a graph based on its neighbors to generate some kind of “fingerprint” or “signature”. With same initialization, we could find correspondance nodes in two graphs when they share the same fingerprint.\nGraph neural network 最为常见的GNN范式可以如此描述.对于图 $G=(V,E)$ , $X_v$ 表示其节点 $v\\in V$ 的特征向量(feature vector)1. 图网络的流程可以简单概括如下： $$ \\begin{aligned} a_v^{(k)}\u0026=\\text{AGGREGATE}^{(k)}\\left( \\left\\{\\left. h_u^{(k-1)}\\right|u\\in \\mathcal{N}(v) \\right\\} \\right), \\\\\nh_v^{(k)} \u0026= \\text{COMGINE}^{(k)}\\left( h_v^{(k-1)}, a_v^{(k)} \\right),\\\\\nh_G \u0026= \\text{READOUT}\\left( \\left\\{ \\left. h_v^{(K)} \\right| v\\in G \\right\\} \\right) \\end{aligned} $$ 其中, $h_v^{(k)}$ 表示节点 $v$ 在第 $k$ 层/循环时的特征向量; $h_v^{(k)}$ 的初始值为 $X_v$ ; $\\mathcal{N}(v)$ 表示节点 $v$ 的邻居节点. 其中三个大写字母表示的函数是区别于不同GNN之间的重要因素. 大概汇总如下:\n GraphSAGE (Hamilton et al., 2017a)  $a_v^{(k)}=\\text{MAX}\\left( \\left\\{\\left. \\text{ReLU}\\left(W\\cdot h_u^{(k-1)}\\right) \\right|u\\in \\mathcal{N}(v) \\right\\} \\right)$ $h_v^{(k)} = W\\cdot \\left[ h_V^{(k-1)}, a_v^{(k)} \\right]$ $\\text{MAX}$ 表示MaxPooling   GCN (Kipf \u0026 Welling, 2017)  $h_v^{(k)}=\\text{ReLU}\\left(W\\cdot \\text{MEAN}\\left\\{ h_u^{(k-1)}, \\forall u\\in \\mathcal{N}(v)\\cup {v} \\right\\}\\right)$ 两个函数合二为一    常见的图上任务有两类：① 节点分类, 每个节点 $v$ 都有一个对应标签 $y_v$, GNN学到了节点表示 $h_v$ 后, 通过映射函数进行预测, 使得 $y_v=f(h_v)$ ; ②图分类, 对于每一个图 $G$, 都有一个标签 $y_G$ , 通过GNN学到图的表示 $h_G$ 后, 通过映射函数进行预测, 使得 $y_G=f(h_G)$.\n工作简述 文章通过理论论证+实验验证的方法指出了做出了两个主要贡献:\n 论证并给出了各种GNN变体的表达能力的上限和达到的条件 通过理论分析设计出了理论上更具优的网络结构GIN  理论证明 这里阐述一个大概的思路. 为了论证GNN的表示能力, 考察何种条件下GNN能将两个节点映射到嵌入空间中相同的位置. 理想情况下, 最具表达能力的GNN将两个节点映射到嵌入空间中相同的位置当且仅当两个节点具有相同的子树结构 (aggregate N 次形成的树). 这表示GNN的 $\\text{AGGREGATE}$ 函数必须是单射的. 而GNN这种区分能力的上限则是WL test. 作者证明,只有当$\\text{AGGREGATE}$ , $\\text{COMBINE}$ 以及 $\\text{READOUT}$ 都是单射的时候, GNN将在描述节点的特征上达到WL test的性能.\n注: 文中这里提出了很多引理, 针对如何设计一个单射的multiset function. 这些是GIN设计及有效的证据.\nGraph Isomorphism Network (GIN) 这一命题构造了一系列符合要求的单设函数. 如果使用MLP学习其中的映射, 那么就得到了GIN的节点表示更新函数就可以写作为下式. 其中MLP可对前一次和下一次的 $\\phi f$ 函数同时建模. 至此, 作者表明他们设计出了一个理论上达到表示能力上界的图神经网络. $$ h_v^{(k)}=\\text{MLP}^{(k)}\\left(\\left( 1+\\epsilon^{(k)} \\right)\\cdot h_v^{(k-1)}+\\sum_{u\\in\\mathcal{N}(v)}h_u^{(k-1)}\\right) $$ 最后, 还差一个 $\\text{READOUT}$ 函数. 作者提出, 在节点对应的遍历树逐渐趋近整个图的时候, 图中的特征将会有更好的判别性能. 但迭代轮数较小时候的特征却可以拥有更好的泛化性能2. 因此作者表示为了能考虑到图中所有结构信息, 我们应当使用所有迭代过程中的特征. 作者通过一个与 Jumping Knowledge Network类似的架构实现了这样的操作: $$ h_G=\\text{CONCAT} \\left( \\text{READOUT}\\left.\\left.\\left(\\left\\{h_v^{(k)}\\right|v \\in G\\right\\} \\right) \\right| k=0,1,\\dots,K \\right) $$ 根据上面的命题, 如果令 $\\text{READOUT}$ 为求和函数, 那么它也将符合单射的条件. 作者在4个生信数据集+5个社交网络数据集上进行了测试, 结果自然是几乎全部怒砍第一.\n关于其他的GNN 其他的GNN虽然没有GIN这么强大,但是同样会捕捉一些图上有趣的性质. 作者针对GCN和GraphSAGE中不满足单设条件的①1层感知器, ②均值或max-pooling而非求和的 $\\text{AGGREGATE}$ 函数做了消融实验. 它们都在一些图结构上无法分辨.\n1层感知器 文章证明了1层感知器 (线性组合+激活函数) 并不能对某些网络结构进行分辨. 即文中引理7:\n Lemma 7. There exist finite multisets $X_1 \\ne X_2$ so that for any linear mapping $W$, $\\sum_{x\\in X_1} \\text{ReLU}(Wx)=\\sum_{x\\in X_2} \\text{ReLU}(Wx)$.\n 证明过程主要思路是, 1层感知器和线性映射区别很小, GNN会退化到每一层仅仅将邻居节点求和. 另外, 众所周知, 1层感知器并不能对任意的函数进行近似.\nMean和Max-pooling 这两个函数虽然都是组合不变的, 但是他们并非单射的. 根据之前的证明, 他们并不能区分某些图结构. 对于Mean函数,很容易能看出来, 它刻画的是周围邻居的均值, 或者可以理解为分布情况. 而对Max-pooling, 它则将原本是Multiset的邻居特征集合退化为一个简单集合, 并使用唯一元素代表所有邻居. 这些结论都有形式化证明, 这里不再赘述.\n结果与总结 这篇论文亮点就在通过WL test分析并设计的GNN确实在下游任务中比原来几种GNN好得多. 这篇文章代码也出奇简单. 有空一定要好好看看, 留个坑.\n  这里特征向量指的并非线性代数中的特征向量. ↩︎\n 作者没有解释, 我也不是很理解这里 ↩︎\n   ","description":"","tags":["Graph Neural Network (GNN)","Machine Learning"],"title":"How Powerful Are Graph Neural Networks?","uri":"/posts/how-powerful-are-graph-neural-networks/"},{"categories":["文献阅读"],"content":" The process is seeded with $N$ vectors …\n From Constrained Graph Variational Autoencoders for Molecule Design\n GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and trans- forming representation vectors of its neighboring nodes.\n  Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs.\n From How Powerful Are Graph Neural Networks?\n Since molecules can be optimized in different ways, there are multiple viable translations for each input graph.\n From Learning Multimodal Graph to Graph Translation for Molecular Optimization\n … converge to brittle solutions\n From Ankesh Anand’s blog\n The goal of drug discovery is to design molecules with desirable chemical properties. The task is challenging since the chemical space is vast and often difficult to navigate.\n From Learning Multimodal Graph to Graph Translation for Molecular Optimization\n One of the hallmarks of deep learning was the use of neural networks with tens or even hundreds of layers. In stark contrast, most of the architectures used in graph deep learning are shallow with just a handful of layers.\n From Michael Bronstein’s blog\n","description":"","tags":["Academic Writing"],"title":"学术英语写作","uri":"/posts/%E5%AD%A6%E6%9C%AF%E8%8B%B1%E8%AF%AD%E5%86%99%E4%BD%9C/"},{"categories":["文献阅读"],"content":"问题提出 文章首先提出迁移学习在CV与NLP中已经应用广泛了，但在图数据上相应的预训练的工作还比较少。预训练主要解决目前图数据集的两个问题：\n task-specific labeled data can be extremely scarce. Graph data from real-world often contain out-of-distribution samples.  第一个问题很好理解，在化学生物领域中，图数据（eg: molecule, protein, etc.）对应的标签的获取过程需要做实验，是资源密集且时间密集的。第二个问题个人理解指的是数据的整体分布和条件分布的区别。例如，所有已发现的分子数据库可看作是整体分布。而对特定的任务如MoleculeNet中的BBBP数据集1，数据的分子则是在某些隐含条件下的，如具有一定的水溶性和脂溶性、常温下不太可能是气体、不太可能含有特定有毒的重金属元素/官能团。\n因此，不少研究都发现，简单地增加有标数据的量并不能一定让预训练或迁移学习进行得更好。相反，预训练需要领域知识来合理地选择和特定下游任务有关的数据。相反，如果下游任务和预训练的任务差别太大，则有可能导致“负迁移”（negative transfer）的问题。\n解决方案 简述 如上图，本文提出使用节点水平+图水平的预训练使得图网络既能学到节点与边层面上的特征（局部特征），也能捕捉到图级别的特征（全局特征）。作者为预训练设计了特定的任务，如上图右边，以此来对特定的图中信息进行建模。本文通过对比使用了比较新GIN模型作为预训练的图网络。\n节点的预训练 承上，节点的预训练作者提出了两种方案，分别针对邻居结构信息和自身节点信息。\n**Context prediction.**对每一个节点 $v$ ， $K$-hop 邻居指该节点出发最多 $K$-hop 以内的所有节点和边。也即是一个常见的 $K$ 层GNN能够搜集信息的范围，对应节点的表示向量 $h_v^{(K)}$ 则取决于它的 $K$-hop 邻居。Context graph 表示一些节点 $v$ 的邻居结构。它由两个参数 $r_1, r_2$ 控制。对于节点 $v$，表示由所有与 $v$ 距离 $r_1$-hop 和 $r_2$-hop 之间的节点和边所构成的子图，可近似看做一个环形区域。令 $r_1 \u003c K$，并将 $K$-hop 邻居和Context graph的交集被称为Context anchor nodes。这一任务如上图(a)所示，第一步先使用一个辅助GNN’来得到Context graph中的节点向量表示，并对Context anchor nodes的表示求平均，得到绿色的向量，对于图 $G$ 中的节点 $v$ ，这样得到的向量为 $c_v^G$。第二步，用主GNN在 $K$-hop 邻居组成的子图上得到 $v$ 的表示 $h_v^{(K)}$。预训练的目标即为： $$ \\sigma(h_v^{(K)}\\cdot c_{v'}^G) \\approx 1 \\;\\;\\text{if $v$ and $v'$ are same node} $$ $\\sigma(\\cdot)$ 表示$\\text{Sigmoid}$函数。第三部，在训练中使用negative sampling，控制 $v'=v$ 或 $G'=G$，让正负样本比例为1。这实际上是要学到图的拓扑结构。\nAttribute masking. 随机mask掉分子图中的一些节点和边的属性，使用GNN预测。这个任务原理没有这么复杂，不再赘述。\n图的预训练 图 $G$ 的向量表示 $h_G$ 下游任务进行微调训练时候直接使用的特征，我们应当让这类特征包含相关的领域知识。作者提出，使用图级别多任务有监督预训练（graph-level multi-task supervised pre-training）来同时预测同一个图的多个标签。但是，如果只是单纯的这样训练，如果预训练的任务与下游任务相关性不强或，则可能出现负迁移的现象。因此，作者认为图的预训练仅仅提供了图层面上的监督，即使节点表示学的很好，但图表示很可能在预训练阶段由于各种原因是没那么有意义的。因此，作者表示要缓解这个问题，就要先进行节点预训练，再进行图的预训练2。另外，作者还说可以用图网络预测图之间的相似性来进行图的预训练。然而他说这个复杂度太高，他不做。\n实验和结果 实验分别在化学任务和生物学任务上进行预训练效果测试。预训练数据来源是ZINC15，只在其中选了2百万个分子3。详细数据和方法略去，仅介绍结果。\n上图是GIN使用不同的与训练策略在下游任务（化学）中的表现，测评指标是ROC-AUC（%）。加粗字体表示最好的几个（best and comparable），灰框的表示出现了负迁移，结果比不预训练的网络还差。\n不同图网络模型在化学和生物两大类下游任务中的表现，比较预训练与否所带来的ROC-AUC（%）的变化，可以轻松看出GIN是最适合预训练的。\n 印证了越复杂的模型就越能更好地利用预训练，相反，越有限的模型就越不能受益于预训练。 仅仅进行了图级别的预训练是不够的，将导致较多负迁移现象（2/8）。 仅仅进行了节点级别的预训练也是不够的，也会导致负迁移现象（1/8）。 结合节点级别和图级别的预训练将能达到最优的下游任务表现，同时消除了负迁移。 作者声称他的方法做到了目前最好4。 与训练模型在下游任务训练时能更快收敛（显然）。    这个数据集是测量一些化合物是否能突破人体血脑屏障的。 ↩︎\n 这不是废话吗😓，反过来的话节点的预训练不就没用了。感觉这里把问题没讲清楚，实际上文中似乎就是把能获取到的数据都拿来预训练。 ↩︎\n 我个人也发现无标签分子的预训练一两百万就差不多很足够了，再多就没有意义了。 ↩︎\n 存疑，这几个数据集合同样是scaffold划分，都没有达到MoleculeNet的给出的结果，不知是如何state-of-the-art的。 ↩︎\n   ","description":"","tags":["Graph Neural Network (GNN)","Machine Learning"],"title":"Strategies for Pre Training Graph Neural Networks","uri":"/posts/strategies-for-pre-training-graph-neural-networks/"},{"categories":["随便写写"],"content":"这是一个一级标题 这是一个二级标题 数学公式 简单公式$A+B=C$这是行内公式 $$ a^2 + b^2 = c^2$$\n$$x=\\frac{-b\\pm\\sqrt{4ac}}{2a}$$ 复杂公式 $$ \\begin{aligned} \\mathcal{L} ( \\mu ,\\sigma^2 ) \u0026= \\prod_{i = 1}^n \\left\\lbrace \\frac{1}{\\sqrt{2 \\pi} \\sigma } \\text{exp} \\left\\lbrace - \\frac{( x_i - \\mu)^2}{2 \\sigma^2}\\right\\rbrace \\right\\rbrace ,\\\\\n\u0026= (2 \\pi \\sigma^2)^{- \\frac{n}{2}} \\text{exp} \\left\\lbrace - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{n} (x_i - \\mu)^2 \\right\\rbrace . \\end{aligned} $$\n公式作为TOC项目 $x=\\frac{-b\\pm\\sqrt{4ac}}{2a}$ $x=\\frac{-b\\pm\\sqrt{4ac}}{2a}$ $x=\\frac{-b\\pm\\sqrt{4ac}}{2a}$ $x=\\frac{-b\\pm\\sqrt{4ac}}{2a}$ 一元二次方程解的公式$x= \\frac{-b\\pm\\sqrt{4ac}}{2a}$\n化学式$\\ce{H3O+}$ $\\ce{[AgCl2]-}$\n$\\ce{NaOH(aq,$\\infty$)}$\n$\\ce{Hg^2+ -\u003e[I-] $\\underset{\\mathrm{red}}{\\ce{HgI2}}$ -\u003e[I-] $\\underset{\\mathrm{red}}{\\ce{[Hg^{II}I4]^2-}}$}$\nEmoji 🌶💉🔟🐮🍺\n代码块 1 2 3 4 5  class Foo(object): def __init__(self, bar): self.foo = bar def print(): print(f\"this is {self.foo}\")   Mermaid graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two] 👉 This is a flowchart 通过shortcode的设置, 当允许图表图例时, 可以通过接收caption参数为mermaid图表加上图例. 但注意要在前后加上空行, 不然这一段就和图表处在同一段了!\n$\\mathbf{A} \\in \\mathbb{R}^{B\\times N \\times N}$\n","description":"","tags":["测试"],"title":"功能测试","uri":"/posts/test-my-site/"}]
